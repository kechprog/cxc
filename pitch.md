# Reflectif — Pitch

## One-Liner

A passive emotional intelligence layer for your conversations — because you don't always know how you feel, but your voice does.

## The Problem

People are bad at recognizing their own emotions in real time. You leave a meeting not realizing you were anxious. You hang up a call not noticing you shut down halfway through. This is literally why therapists exist — they catch what you can't. But therapy covers one hour a week. The other 100+ hours of conversation go unexamined.

Current tools don't help. Journaling is retroactive and filtered through your own blind spots. Mood trackers rely on self-reporting, which is exactly the problem. Chat-based AI can only analyze what you *type*, not how you *feel*.

## The Solution

Record a conversation. Get back an emotional breakdown with insights you'd never catch yourself — powered by vocal analysis, not just words. Over time, track your EQ growth like a fitness app tracks your health.

**What you get after each conversation:**
- An overall emotional read (emoji + label like "Tense" or "Warm")
- A dynamic breakdown of how the conversation evolved phase by phase — where things shifted, why, and what you could do differently
- Communication patterns you're exhibiting ("deflects with humor when challenged")

**What you get over time:**
- EQ progress tracking across research-backed dimensions (self-awareness, self-regulation, empathy, social skills, motivation)
- Concrete evidence of what's improving and what still needs work
- An AI therapist you can chat with anytime — it knows your history, your patterns, and your goals

**What makes it different:**
- Analyzes your *voice*, not your words — your tone reveals what your language hides
- Tracks how you affect others, not just how you feel — bidirectional emotional awareness
- Not a mood tracker. An EQ trainer. Feeling bad isn't a negative signal. Handling it poorly is.

## Bidirectional Emotional Tracking

The app tracks sentiment for **all speakers**, not just the user. You don't live in a vacuum.

You call your friend stupid because you assume they don't care. But the model detects hurt in their voice — their pitch drops, they go quiet, their energy flatlines for the rest of the conversation. You didn't notice in the moment. The AI did.

The post-conversation summary flags this and suggests you check in with them. This is the insight most people never get without a therapist or a very honest friend. The app becomes a mirror for how you show up in other people's experience, not just your own.

## User Experience

1. **Onboarding**: A brief AI-guided conversational interview (2-3 min) to understand your background, goals, and relationships. This also captures your voice for speaker identification in future recordings.
2. **Record**: Enable recording for a conversation (phone call, in-person, meeting).
3. **Analyze**: After the conversation, see the emotional breakdown — phases, insights, patterns.
4. **Chat**: Talk to the AI therapist anytime. Discuss a specific insight, explore a pattern, or just check in. It knows your history.
5. **Grow**: Over time, track your EQ progress. See concrete evidence of behavioral change.

## Privacy Model

- Recording is entirely user-initiated — the user is responsible for obtaining consent from other participants
- Liability for recording sits with the user, not the app
- The app does not share, transmit, or store audio beyond the user's own account
- This is the same model used by voice memo apps and call recorders

## Demo Strategy

The demo should center on **one powerful moment**: a conversation where someone says something positive but the vocal analysis catches a negative emotion underneath.

- "No, I'm fine with that" → model flags anxiety at 0.7
- Show the emotional timeline spiking at that exact moment
- Show the AI summary calling it out: "You agreed verbally but showed signs of discomfort"

This is the "wow" moment. Build the entire demo around making this single moment as clear and visceral as possible.

### Pitch Flow (3 minutes)

1. **The hook** (20 sec): "You don't always know how you feel. But your voice does."
2. **The problem** (30 sec): People miss their own emotional signals. Self-reporting is broken. Therapists catch this — but only 1 hour a week.
3. **Live demo** (90 sec): Show a conversation being analyzed. Highlight the moment where words and voice diverge. Show the emotional timeline. Show the AI summary.
4. **The long game** (20 sec): Show the EQ progress — "over the past 2 weeks, your self-awareness score improved. Here's the evidence."
5. **Close** (20 sec): "Your voice already knows how you feel. We just help you listen."

## Judging Criteria Alignment

| Criteria | How We Hit It |
|---|---|
| Innovation | Passive voice-based EQ training is not available in consumer tools today |
| Technical Complexity | GPU-accelerated voice identification + AudioPod speaker splitting + Hume.ai 48-emotion analysis + LLM synthesis + RAG memory — not a 1-prompt project |
| Real-World Application | Universal — anyone who talks to people can use this. Mental health, professional development, relationships |
| "Wow" Factor | The moment the model catches an emotion your words hid is visceral and immediately relatable |
